import requests
import pandas as pd
import json
from pyspark.sql import SparkSession
from pyspark.sql.functions import lit, current_timestamp, col
from pyspark.sql.types import IntegerType
from datetime import datetime
import os

# -------------------------------
# 1. Fetch SharePoint List Items
# -------------------------------
def get_sharepoint_list_data(client_id, client_secret, tenant_id, site_url, list_name):
    token_url = f"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token"
    token_data = {
        'grant_type': 'client_credentials',
        'client_id': client_id,
        'client_secret': client_secret,
        'scope': 'https://graph.microsoft.com/.default'
    }
    token_response = requests.post(token_url, data=token_data)
    token_response.raise_for_status()
    access_token = token_response.json().get('access_token')

    headers = {
        'Authorization': f'Bearer {access_token}',
        'Accept': 'application/json',
        'Content-Type': 'application/json'
    }

    site_info_url = f"https://graph.microsoft.com/v1.0/sites/root:/{site_url}"
    site_response = requests.get(site_info_url, headers=headers)
    site_response.raise_for_status()
    site_data = site_response.json()
    site_id = site_data['id']

    list_url = f"https://graph.microsoft.com/v1.0/sites/{site_id}/lists?$filter=displayName eq '{list_name}'"
    list_response = requests.get(list_url, headers=headers)
    list_response.raise_for_status()
    list_id = list_response.json()['value'][0]['id']

    all_items = []
    items_url = f"https://graph.microsoft.com/v1.0/sites/{site_id}/lists/{list_id}/items?$expand=fields"
    while items_url:
        response = requests.get(items_url, headers=headers)
        response.raise_for_status()
        items_data = response.json()
        all_items.extend(items_data.get("value", []))
        items_url = items_data.get("@odata.nextLink")

    return all_items

# -------------------------------
# 2. Flatten Items to DataFrame
# -------------------------------
def process_sharepoint_data(response_data):
    processed_items = []

    for item in response_data:
        all_fields = item.copy()
        fields = item.get('fields', {})
        for key, value in fields.items():
            if isinstance(value, dict):
                for subkey, subvalue in value.items():
                    all_fields[f"{key}_{subkey}"] = subvalue
            else:
                all_fields[key] = value
        processed_items.append(all_fields)

    df = pd.DataFrame(processed_items)
    for col_name in df.columns:
        df[col_name] = df[col_name].apply(lambda x: json.dumps(x) if isinstance(x, dict) else x)
    return df

# -------------------------------
# 3. Incremental Filter
# -------------------------------
def filter_incremental(df, last_modified_col, checkpoint_path):
    if os.path.exists(checkpoint_path):
        with open(checkpoint_path, 'r') as f:
            last_processed = f.read().strip()
    else:
        last_processed = "1900-01-01T00:00:00Z"

    filtered_df = df.filter(col(last_modified_col) > last_processed)
    max_timestamp = filtered_df.agg({"lastModifiedDateTime": "max"}).collect()[0][0]

    if max_timestamp:
        with open(checkpoint_path, 'w') as f:
            f.write(max_timestamp)

    return filtered_df

# -------------------------------
# 4. Entry Point
# -------------------------------
if __name__ == "__main__":
    client_id = "YOUR_CLIENT_ID"
    client_secret = "YOUR_CLIENT_SECRET"
    tenant_id = "YOUR_TENANT_ID"
    sharepoint_url = "https://yourdomain.sharepoint.com/sites/SiteName"
    list_name = "Your List Name"
    checkpoint_path = "/tmp/sharepoint_checkpoint.txt"  # Or use S3 path via boto3 or DB
    raw_s3_path = "s3a://your-bucket/path/to/raw/"

    site_path = '/'.join(sharepoint_url.split('/')[3:])
    raw_data = get_sharepoint_list_data(client_id, client_secret, tenant_id, site_path, list_name)
    pandas_df = process_sharepoint_data(raw_data)

    spark = SparkSession.builder.appName("SharePointIngest").getOrCreate()
    spark_df = spark.createDataFrame(pandas_df)

    # Cast lastModifiedDateTime as timestamp
    spark_df = spark_df.withColumn("lastModifiedDateTime", col("lastModifiedDateTime").cast("timestamp"))

    # Apply incremental filtering
    spark_df = filter_incremental(spark_df, "lastModifiedDateTime", checkpoint_path)

    # Reorder and enrich columns
    cols = spark_df.columns
    cols = ["op", "insrt_dtm"] + [c for c in cols if c not in ["op", "insrt_dtm", "sequence_number"]] + ["sequence_number"]
    spark_df = spark_df.withColumn("op", lit("U")) \
                       .withColumn("insrt_dtm", current_timestamp()) \
                       .withColumn("sequence_number", lit(None).cast(IntegerType())) \
                       .select(*cols)

    # Save to S3
    spark_df.write.mode("overwrite").parquet(raw_s3_path)
